{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 07 - Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 07 - Advanced RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.unsplash.com/photo-1548018628-c56f29b810b4?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1041&q=80)\n",
    "Picture by [Taras Chernus](https://unsplash.com/photos/uQMyw1VFKqI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Today, we keep diving into the Recurrent Neural Networks. We will learn about more advanced techniques to handle long term dependencies, and more types of RNN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Translation Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.1. How to translate a sentence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most intuitive way to translate a sentence would be to use one of the RNN types we saw yesterday: the many-to-many:\n",
    "![](images/many-to-many.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the idea is that you take a sentence in english:\n",
    "> `My name is Cassidy.`\n",
    "\n",
    "If we have a well trained network, this should word :\n",
    "The first word `My` is given as input, and outputs `Mon`.\n",
    "\n",
    "The second word `name` outputs `nom`, `is` outputs `est`, and finally `Cassidy` outputs `Cassidy`. Our translation will be \n",
    "> `Mon nom est Cassidy.`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's think about it a bit more: would it work every time doing that way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2. Swapping words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider the following sentence, and let's do the same exercise:\n",
    "> `What is your favorite movie?`\n",
    "\n",
    "The translation now would be the following:\n",
    "> `Quel est ton préféré film ?`\n",
    "\n",
    "This is really **not good**, we want the following translation:\n",
    "> `Quel est ton film préféré ?`\n",
    "\n",
    "\n",
    "Because english and french do not always order the same adjectives and nouns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to deal with that problem? The neural network (and even a human!) actually has to know that both words `favorite` and `movie` have to be translated!\n",
    "\n",
    "When you human, want to translate a sentence, don't you read it all first? Then let's give our RNN a chance to do the same! This would give the following architecture:\n",
    "\n",
    "![](images/many-to-many_translation.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this architecture mean? Well we have two sides.\n",
    "\n",
    "The left side, we feed our RNN with the sequence of words in english. So that the RNN has a chance to \"read\" **all the sequence** before making any translation. Just like a human does!\n",
    "\n",
    "The right side is a bit trickier. Once all the sentence has been read, the RNN begins to predict translation. So it begins by predicting the first word, the same way we did up to now.\n",
    "\n",
    "Then, the predicted word is fed as input to predict the next word. Indeed, the **RNN uses the previous predicted word to predict the next one**!\n",
    "\n",
    "This architecture is sometimes called an **encoder-decoder sequence-to-sequence** model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.3. Long term dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look at a final translation example. We want to translate this sentence\n",
    "> `Cats, unlike my dog, don't like water.`\n",
    "\n",
    "Into this one:\n",
    "> `Les chats, contrairement à mon chien, n'aiment pas l'eau.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would perform our new architecture on that sentence? Quite good, but for sure it would **miss the plural agreement** and give the following translation:\n",
    "> `Les chats, contrairement à mon chien, n'aime pas l'eau.`\n",
    "\n",
    "Indeed, our RNN would predict `n'aime pas` based on `mon chien` right before. While actually, it should remember that it is related to `Les chats` a long time ago...\n",
    "\n",
    "This is the problem of **long-term dependencies**. To manage them, we would like to **keep memory** of the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Memory Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.1. RNN Cell structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will change our representation now, so as a reminder, here is how we could see a RNN cell:\n",
    "![](images/RNN_cell.png)\n",
    "\n",
    "Where we find back what we already know:\n",
    "- The hidden state $h_t$ is computed using $h_{t-1}$ and $x_t$\n",
    "- The prediction $\\hat{y}$ is computed using $h_t$\n",
    "\n",
    "Nothing new for now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.2. Gated Recurrent Unit structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep memory of older state, the first idea was to create an **update gate** usually noted $\\Gamma_u$. This is a **Gated Recurrent Unit** (often called **GRU**) \n",
    "\n",
    "The idea of this update gate is to have a variable that says how much information from the past step we should keep.\n",
    "\n",
    "The structure would be the following:\n",
    "![](images/GRU_cell.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see on the diagram, the update gate is computed using the sigmoid function, using the same inputs as a regular RNN:\n",
    "\n",
    "$$\n",
    "\\large \\Gamma_u = sigmoid(W_{hu} h_{t-1} + W_{xu} X_t + b_u)\n",
    "$$\n",
    "\n",
    "This value $\\Gamma_u$ can really be seen as a gate:\n",
    "* If $\\Gamma_u = 1$, completely forget the previous state\n",
    "* If $\\Gamma_u = 0$, do not update at all, keep only the previous state\n",
    "\n",
    "This should allow our network to remember information on longer sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Optional*: For those who want to full mathematics behind, here they are:\n",
    "\n",
    "$$\n",
    "\\hat{h_t} = g(W_{hh} \\Gamma_r h_{t-1} + W_{xh} X_t + b_h)\n",
    "$$\n",
    "\n",
    "$$\n",
    " \\Gamma_u = sigmoid(W_{hu} h_{t-1} + W_{xu} X_t + b_u)\n",
    "$$\n",
    "\n",
    "$$\n",
    " \\Gamma_r = sigmoid(W_{hr} h_{t-1} + W_{xr} X_t + b_r)\n",
    "$$\n",
    "\n",
    "$$\n",
    " \\hat{h_t} = \\Gamma_u \\hat{h_t} + (1-\\Gamma_u)h_{t-1}\n",
    "$$\n",
    "\n",
    "Where $\\Gamma_r$ can be seen as a relevance gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No worries**, as usual, all of this is **already coded in TensorFlow**, all you will have to do is:\n",
    "```Python\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(units=16))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.3. Long Short-Term Memory Cell "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Historically, before creating GRU, Long Short-Term Memory (LSTM) cells were invented. They are even more complex than GRU.\n",
    "\n",
    "LSTM cells allow to keep information over longer sequences than GRU cells. The drawback is that they have much more parameters, and are thus harder to train: **LSTM need a larger training dataset**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GRU cell can be seen a binary update gate: you can either keep all the past information, or rely only on current information. (Even though it is more a threshold that can keep half past and half current information.)\n",
    "\n",
    "A LSTM offers much more liberty: you can either keep all past information **and** all current information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure is the following:\n",
    "\n",
    "![](images/LSTM_cell.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure seems now very complicated, let's have a look!\n",
    "\n",
    "We still have the update gate that keeps working the same:\n",
    "\n",
    "$$\n",
    "\\Gamma_u = sigmoid(W_{hu} h_{t-1} + W_{xu} X_t + b_u)\n",
    "$$\n",
    "\n",
    "And there is now the **forget gate** $\\Gamma_f$ that is computed the same way (with different weights of course!):\n",
    "\n",
    "$$\n",
    "\\Gamma_f = sigmoid(W_{hf} h_{t-1} + W_{xf} X_t + b_f)\n",
    "$$\n",
    "\n",
    "And we still compute an intermediary state, called now $\\hat{c}$ instead of $\\hat{h}$ before, but this is pretty much the same for the moment:\n",
    "\n",
    "$$\n",
    "\\hat{c_t} = g(W_{hc} \\Gamma_r h_{t-1} + W_{xc} X_t + b_c)\n",
    "$$\n",
    "\n",
    "There is a new player now, the **cell state** $c_t$, that is computed thanks to the update gate, the forget gate and the current state:\n",
    "\n",
    "$$\n",
    "c_t = \\Gamma_f c_{t-1} + \\Gamma_u \\hat{c}_t\n",
    "$$\n",
    "\n",
    "Finally, there is one more gate computed like other gates, the **output gate** $\\Gamma_o$:\n",
    "\n",
    "$$\n",
    "\\Gamma_o = sigmoid(W_{ho} h_{t-1} + W_{xo} X_t + b_o)\n",
    "$$\n",
    "\n",
    "That allows to compute the final hidden state $h_t$:\n",
    "\n",
    "$$\n",
    "h_t = \\Gamma_o c_t\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the GRU, the LSTM is **coded in TensorFlow**, all you will have to do is:\n",
    "```Python\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=16))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Improving the movie review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using RNN cells, we reached about 80% accuracy score on the movie review dataset. Can we do better using GRU and/or LSTM?\n",
    "\n",
    "Let's try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets\n",
    "imdb = datasets.imdb\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train,\n",
    "                                 value=0,\n",
    "                                 padding='post',\n",
    "                                 maxlen=256)\n",
    "\n",
    "X_test = sequence.pad_sequences(X_test,\n",
    "                                value=0,\n",
    "                                padding='post',\n",
    "                                maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GRU\n",
    "\n",
    "# Define the model\n",
    "def my_RNN():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=10000, output_dim=32, input_length=256))\n",
    "\n",
    "    # We add two layers of GRU \n",
    "    model.add(GRU(units=24, return_sequences=True))\n",
    "    model.add(GRU(units=24, return_sequences=False))\n",
    "\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model = my_RNN()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 55s 2ms/sample - loss: 0.3121 - acc: 0.8787 - val_loss: 0.4082 - val_acc: 0.8321\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 58s 2ms/sample - loss: 0.2300 - acc: 0.9176 - val_loss: 0.3907 - val_acc: 0.8399\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 60s 2ms/sample - loss: 0.1791 - acc: 0.9426 - val_loss: 0.4032 - val_acc: 0.8433\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.1464 - acc: 0.9566 - val_loss: 0.4286 - val_acc: 0.8447\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 56s 2ms/sample - loss: 0.1219 - acc: 0.9672 - val_loss: 0.4631 - val_acc: 0.8415\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.1041 - acc: 0.9737 - val_loss: 0.5062 - val_acc: 0.8410\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 58s 2ms/sample - loss: 0.0938 - acc: 0.9765 - val_loss: 0.5006 - val_acc: 0.8418\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 58s 2ms/sample - loss: 0.0815 - acc: 0.9811 - val_loss: 0.5444 - val_acc: 0.8374\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 58s 2ms/sample - loss: 0.0760 - acc: 0.9818 - val_loss: 0.5395 - val_acc: 0.8396\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 61s 2ms/sample - loss: 0.0741 - acc: 0.9816 - val_loss: 0.5598 - val_acc: 0.8381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7c1451e278>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 34s 1ms/sample - loss: 0.0704 - acc: 0.9823\n",
      "accuracy on train with NN: 0.98232\n",
      "25000/25000 [==============================] - 37s 1ms/sample - loss: 0.5598 - acc: 0.8381\n",
      "accuracy on test with NN: 0.83808\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('accuracy on train with NN:', model.evaluate(X_train, y_train)[1])\n",
    "print('accuracy on test with NN:', model.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GRU, we could now reach about 84%, quite an improvement! And there is room for improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. More RNNs (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV.1. Types of RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yesterday you have seen some types of RNNs, especially you know there is many-to-many and many-to-one.\n",
    "![](images/RNN_types.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to compute that difference with TensorFlow?\n",
    "\n",
    "With the parameter `return_sequences`:\n",
    "- When you set `return_sequences = False` to the last layer of your RNN, it won't return anything: thus this is a many-to-one.\n",
    "- When you set `return_sequences = True` to the last layer of your RNN, it will return an output prediction for each step: this is a many-to-many"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, there are more types of RNNs, you already saw the translation one (the encoder-decoder sequence-to-sequence model), a many-to-many with shift.\n",
    "\n",
    "You can also do a **One-to-Many** RNN:\n",
    "![](images/one-to-many.png)\n",
    "\n",
    "This can be used for example in music generation! Or any type of sequence generation! They are a bit more complicated to compute using TensorFlow, but it is possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV.2. Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you might want your RNN to read a sequence in both ways: from the beginning to the end, but also from the end to the beginning. In some cases, it can help it catch relationships between time steps that are in a way but not another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is fairly easy using TensorFlow to compute such a network:\n",
    "```Python\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(units=16)))\n",
    "```\n",
    "\n",
    "See, really easy, and yet sometimes very powerful!\n",
    "\n",
    "But warning, this multiplies by two the computation time!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
