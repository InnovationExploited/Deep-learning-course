{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 07 - Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 06 - RNN Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.unsplash.com/photo-1516476675914-0160447c7282?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1035&q=80)\n",
    "Picture by [Martin Adams](https://unsplash.com/photos/lxujDxNigL4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Today is about another widely used kind of neural networks: the Recurrent Neural Networks. They are used in many modern applications requiring to handle sequences of information, such as language translation or speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. A Sequential Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.1. What is sequential information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a typical problem: you have an image of a ball in motion, like the following, can you predict where the ball will go next?\n",
    "\n",
    "![](images/ball_alone.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless you know newtonian physics better than any current scientist, you can not predict where the ball will go.\n",
    "\n",
    "But now assume that you have some previous positions of the ball as well, like in the following image, can you predict the direction?\n",
    "\n",
    "![](images/ball_with_previous.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure you can, because you have a sequential information: not just a snapshot a given time, but **several snapshots** at different times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2. Examples of sequential problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are playing everyday with sequential information, without even noticing it!\n",
    "\n",
    "For example, audio signals (music, speech, any sound...) are sequential information.\n",
    "\n",
    "![](images/Audio-Waveforms.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An audio signal is just a long list of numbers, they represent a sequence of amplitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another good example is what you are just reading... Yeah, **a sentence is a sequence of words**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, there are numbers of sequential problems:\n",
    "* Language translation\n",
    "* speech recognition\n",
    "* music generation\n",
    "* sentiment classification\n",
    "* video activity\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.3. Limitations of classical approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a simple case. We have the following problem:\n",
    "\n",
    "![](images/cat_sentence.png)\n",
    "\n",
    "How would you solve it using machine learning and NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea 1**: a window of few previous words and one hot encoding:\n",
    "![](images/idea1.png)\n",
    "\n",
    "\n",
    "This is a good idea, but what if we have long term dependencies, like in a sentence like the following:\n",
    "> \"I used to live in China when I was a kid, even though I'm French. That's why I speak fluent ...\"\n",
    "\n",
    "Would that that on this kind of sentence? Wouldn't it predict that the guy speaks French while it is meant to predict Chinese?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea 2**: a BOW (or TF-IDF):\n",
    "![](images/idea2.png)\n",
    "\n",
    "BOW and TF-IDF are really powerful features. But they **do not preserve order** of the sequence. For a BOW, the two following sentences are the same:\n",
    "\n",
    "> \"The food is **good**, **not bad** at all!\"\n",
    "\n",
    "> \"The food is **bad**, **not good** at all!\"\n",
    "\n",
    "This is a serious issue when dealing with sentiment analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# II. RNN Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.1. Requirements to handle sequential information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly handle sequential information, here is what we would need to do:\n",
    "- handle variable length sequences\n",
    "- track long-term dependencies\n",
    "- keep information about order\n",
    "- share parameters across the sequence\n",
    "\n",
    "Well, **Recurrent Neural Networks (RNN)** can do most of it, let's see how!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.2. Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, you used to see diagrams where neural networks are going from left (with the features) to right (with the prediction).\n",
    "\n",
    "![](images/MLP_with_activations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using recurrent neural networks, we will change this representation to the following:\n",
    "![](images/RNN_representation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where in blue are the input features, in green here are the layers of the neural network, and in purple the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.3. Types of RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the problem to solve, many types of RNN can be used. Indeed, a sentiment analysis (one ouput) or a language translation (multiple output) have different requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/RNN_types.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to summarize:\n",
    "- A one-to-one is a MLP as you already know it\n",
    "- A many-to-many could be a translation model: it inputs a sequence of words in english and outputs a sequence of words in french\n",
    "- A many-to-one could be a sentiment analysis model: it inputs a sequence of words and outputs a review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.4. Hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before digging into the details of the computation, one more concept to add is the **hidden state**.\n",
    "\n",
    "![](images/hidden_state.png)\n",
    "\n",
    "Actually, as you will see in the next section, a RNN has a two step computation. First, you compute a **hidden state** $h_t$ using both:\n",
    "- the input features $x_t$\n",
    "- the previous hidden state $h_{t-1}$\n",
    "\n",
    "After that only, you compute the prediction $\\hat{y}$ using this hidden state $h_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. RNN Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.1. Step-by-step computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how does a RNN computes predictions?\n",
    "\n",
    "We will consider $x_1$, $x_2$, $x_3$... $x_t$ to be the words number 1, 2, 3... t of a sentence in english.\n",
    "\n",
    "The target $y_1$, $y_2$, $y_3$... $y_t$ are the words number 1, 2, 3... t of the same sentence in french.\n",
    "\n",
    "So our neural network will look like this:\n",
    "![](images/RNN_ex1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a RNN, actually the same weights are shared for all steps of the sequence:\n",
    "\n",
    "![](images/RNN_ex3.png)\n",
    "\n",
    "The weights $W_{xh}$ and $W_{hh}$ will allow to compute the hidden state as a perceptron would do:\n",
    "\n",
    "$$\n",
    "\\large h_t = g(W_{hh} h_{t-1} + W_{xh} X_t + b)\n",
    "$$\n",
    "\n",
    "This is just like a perceptron, or a classical neural network, where $W_{xh} X_t$ is a weighted sum of features $X_t$, and $g$ is just an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the second step will be to compute the predictions $\\hat{y}$.\n",
    "\n",
    "![](images/RNN_ex4.png)\n",
    "\n",
    "This is where the weights $W_{hy}$ appear. Using those weights, this will again work just like a perceptron:\n",
    "\n",
    "$$\n",
    "\\large \\hat{y_t} = g(W_{hy} h_{t} + b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.2. Loss computation (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be wondering... how do we compute the loss in such a complicated network? And thus, what do we minimize with gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time step is a regular neural network (a MLP). So, we can compute a loss for each time step, right?\n",
    "\n",
    "![](images/RNN_losses1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that at each step *t*, we end up with a loss $L_t$. Then, sum them and you will have the global loss!\n",
    "![](images/RNN_losses2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Application to movie review analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV.1. Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IMDB movie review dataset is a dataset containing review for movies, as well as an associated label 0 (negative review) or 1 (positive review).\n",
    "\n",
    "Let's load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import datasets\n",
    "imdb = datasets.imdb\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible labels: [0 1]\n"
     ]
    }
   ],
   "source": [
    "print('possible labels:', np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train set is composed of 25000 samples. In each training sample, there is a list of numbers, corresponding to the output of a BOW with 10000 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the training set actually is a sequence of words, encoded into numbers. In the first exercise, you will be able to find the correspondences between those numbers and the corresponding words.\n",
    "\n",
    "\n",
    "The dataset is already preprocessed, which is highly convenient. But still, the sequences may not have the same length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of sequence 0: 218\n",
      "length of sequence 1: 189\n"
     ]
    }
   ],
   "source": [
    "print('length of sequence 0:', len(X_train[0]))\n",
    "print('length of sequence 1:', len(X_train[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to pad our training set. This is exactly what we did with images in CNN: we will add zeros so that all the sequences have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train,\n",
    "                                 value=0,\n",
    "                                 padding='post', # to add zeros at the end\n",
    "                                 maxlen=256) # the length we want\n",
    "\n",
    "X_test = sequence.pad_sequences(X_test,\n",
    "                                value=0,\n",
    "                                padding='post', # to add zeros at the end\n",
    "                                maxlen=256) # the length we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of sequence 0: 256\n",
      "length of sequence 1: 256\n"
     ]
    }
   ],
   "source": [
    "print('length of sequence 0:', len(X_train[0]))\n",
    "print('length of sequence 1:', len(X_train[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this padding properly, we would first have to check the distribution of the length of our sequences.\n",
    "Indeed, if most sequences have a length below 80, it makes no sense to keep a length of 256. On the other hand, if most sequences are 800 words long, a 256 padding will lose most of the information.\n",
    "\n",
    "You will implement this in the exercises.\n",
    "\n",
    "Now we are ready to build our first RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV.2. Building a RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build a RNN to predict the labels. We can build a RNN with two layers and 8 units each. Finally we will add a sigmoid dense layer that computes the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding\n",
    "\n",
    "\n",
    "def my_RNN():\n",
    "\n",
    "    model = Sequential()\n",
    "    # The input_dim is the number of different words we have in our corpus: here 10000\n",
    "    # The input_length is the length of our sequences: here 256 thanks to padding\n",
    "    model.add(Embedding(input_dim=10000, output_dim=32, input_length=256))\n",
    "\n",
    "    # We add two layers of RNN \n",
    "    model.add(SimpleRNN(units=8, return_sequences=True))\n",
    "    model.add(SimpleRNN(units=8, return_sequences=False))\n",
    "    \n",
    "    # Finally we add a sigmoid\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several things can be noticed here:\n",
    "- We have an `Embedding` layer: it converts our BOW input into Word Embedding like numbers, so here basically each word is transformed into a 32 features array\n",
    "- We have stacked two `RNN` layers: this does not mean we are doing two RNN, this just means our neural network contains two layers\n",
    "- This first `RNN` has `return_sequences=True` and the second one has `return_sequences=False`: indeed the sequence is needed when another layer of `RNN` is added. The thumb rule for *many-to-one* is: `return_sequences=True` when there is another `RNN` layer, `return_sequences=False` otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example, we will take only one layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_RNN():\n",
    "\n",
    "    model = Sequential()\n",
    "    # The input_dim is the number of different words we have in our corpus: here 10000\n",
    "    # The input_length is the length of our sequences: here 256 thanks to padding\n",
    "    model.add(Embedding(input_dim=10000, output_dim=32, input_length=256))\n",
    "\n",
    "    # We add one layers of RNN \n",
    "    model.add(SimpleRNN(units=32, return_sequences=False))\n",
    "    \n",
    "    # Finally we add a sigmoid\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you already know, we can then compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_RNN()\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's train our RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 12s 468us/sample - loss: 0.6910 - acc: 0.5191 - val_loss: 0.6855 - val_acc: 0.5288\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 13s 516us/sample - loss: 0.6445 - acc: 0.6080 - val_loss: 0.6111 - val_acc: 0.6930\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 13s 502us/sample - loss: 0.5259 - acc: 0.7552 - val_loss: 0.5124 - val_acc: 0.7859\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 13s 520us/sample - loss: 0.5137 - acc: 0.7575 - val_loss: 0.7226 - val_acc: 0.5774\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 14s 554us/sample - loss: 0.4304 - acc: 0.8264 - val_loss: 0.4768 - val_acc: 0.8099\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 14s 544us/sample - loss: 0.4113 - acc: 0.8353 - val_loss: 0.5486 - val_acc: 0.7841\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 14s 561us/sample - loss: 0.4005 - acc: 0.8419 - val_loss: 0.6077 - val_acc: 0.7222\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 14s 567us/sample - loss: 0.3765 - acc: 0.8534 - val_loss: 0.7082 - val_acc: 0.6812\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 15s 582us/sample - loss: 0.3979 - acc: 0.8324 - val_loss: 0.5301 - val_acc: 0.7812\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 14s 558us/sample - loss: 0.3433 - acc: 0.8683 - val_loss: 0.5369 - val_acc: 0.7834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe4c62b3748>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 7s 263us/sample - loss: 0.2924 - acc: 0.8937\n",
      "accuracy on train with NN: 0.89372\n",
      "25000/25000 [==============================] - 9s 345us/sample - loss: 0.5369 - acc: 0.7834\n",
      "accuracy on test with NN: 0.7834\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('accuracy on train with NN:', model.evaluate(X_train, y_train)[1])\n",
    "print('accuracy on test with NN:', model.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is quite high for such a simple RNN: it took only a few lines of code and a couple of minutes to reach a 78 % accuracy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
