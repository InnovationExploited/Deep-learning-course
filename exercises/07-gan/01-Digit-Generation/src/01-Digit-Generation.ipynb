{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Digit Generation ðŸ”¢\n",
    "\n",
    "---\n",
    "\n",
    "![](https://images.unsplash.com/photo-1502570149819-b2260483d302?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1050&q=80)\n",
    "\n",
    "Photo by [Nick Hillier](https://unsplash.com/photos/yD5rv8_WzxA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A GAN to generate digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will be asked to :\n",
    "- complete the code to create your first GAN\n",
    "- train a GAN to generate digits based on the MNIST dataset\n",
    "\n",
    "You should be able to generate new digits by the end of the exercise. This exercise can be run locally, but you can also go for a notebook in Google Colab for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:24:28.715849Z",
     "start_time": "2019-05-30T12:24:28.706796Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.image as mpimg\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T16:33:17.276747Z",
     "start_time": "2019-05-29T16:33:17.272840Z"
    }
   },
   "source": [
    "## I. Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this GAN example, we're going to use the MNIST dataset. MNIST is a set of handwritten digits. We'll try to generate new digit samples using GANs.\n",
    "\n",
    "We kindly remind you how to load the data ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:24:30.426759Z",
     "start_time": "2019-05-30T12:24:29.727444Z"
    }
   },
   "outputs": [],
   "source": [
    "(X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "# Some useful variables\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**. Rescale the data from -1 to 1 and format the X_train dataset in order to have the proper dimensions\n",
    "\n",
    "> ðŸ”¦ **Hint**: Remember, the MNIST dataset is grayscale so contains only one channel but Keras expects input images to have 3 dimensions even if there is only one channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:24:30.426759Z",
     "start_time": "2019-05-30T12:24:29.727444Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Rescale -1 to 1 and format the X_train dataset\n",
    "### STRIP_START ###\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**.Visualize one of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:24:30.753018Z",
     "start_time": "2019-05-30T12:24:30.440877Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAABTJJREFUeJzt3SFulF0bgOFO8wkSBBAQBAMrICygAtEQSBAoLGwAR1KDwZGgCAFbgkewAAKYhiUUQ4JAFIVAIfr+G2BO83c6U9r7uuQ8mXPG3D1JT+ad2TRNa8Dpt37cHwBYDbFDhNghQuwQIXaI+G+Vm81mM//6hyWbpmn2t9ed7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHiP+O+wPAPE+ePBnOnz59Opyvr88/y27evDl87+fPn4fzk8jJDhFihwixQ4TYIULsECF2iBA7RLhn59g8fPhwON/a2hrO9/f3D733NE2Hfu9J5WSHCLFDhNghQuwQIXaIEDtEuHrj2Fy9enU4P3PmzIo+SYOTHSLEDhFihwixQ4TYIULsECF2iHDPzlJtbm7OnT169GihtXd3d4fzu3fvzp3t7e0ttPdJ5GSHCLFDhNghQuwQIXaIEDtEiB0i3LOzkI2NjeF8e3t77uzcuXML7f38+fPh/Pv37wutf9o42SFC7BAhdogQO0SIHSLEDhFihwj37CzkwYMHw/mVK1cOvfanT5+G87dv3x567SInO0SIHSLEDhFihwixQ4TYIULsEDGbpml1m81mq9uMI3Hp0qXh/KDnr+/v78+d/fr1a/je+/fvD+cfP34czqumaZr97XUnO0SIHSLEDhFihwixQ4TYIcJXXOOuXbs2nL97925pe798+XI4d7V2tJzsECF2iBA7RIgdIsQOEWKHCLFDhHv2uNu3bw/n169fX2j9Dx8+zJ29ePFiobX5/zjZIULsECF2iBA7RIgdIsQOEWKHCI+SPuXu3bs3nL9582Y4P3v27HC+s7MznI8eB33QY6g5HI+ShjixQ4TYIULsECF2iBA7RIgdInyf/RQYPft9mc99X1tbW/v27dtw7i793+FkhwixQ4TYIULsECF2iBA7RIgdItyznwJbW1tzZ/v7+0vd+9mzZ0tdn6PjZIcIsUOE2CFC7BAhdogQO0S4ejsBbty4MZzfunVraXu/f/9+OP/69evS9uZoOdkhQuwQIXaIEDtEiB0ixA4RYocIP9l8Avz8+XM4v3DhwqHX/vLly3B+586d4fz379+H3pvl8JPNECd2iBA7RIgdIsQOEWKHCLFDhO+znwAXL14czhd5XPTr16+Hc/fop4eTHSLEDhFihwixQ4TYIULsECF2iHDP/g/Y3t4eztfXl/c3eWdnZ2lr829xskOE2CFC7BAhdogQO0SIHSJcva3AQT+5vLm5OZwf9BXWP3/+zJ29evVq+N69vb3hnNPDyQ4RYocIsUOE2CFC7BAhdogQO0S4Z1+B8+fPD+eXL19eaP0fP37MnT1+/HihtTk9nOwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4Tvs6/A7u7ucH7QzyZvbGwc5cchyskOEWKHCLFDhNghQuwQIXaIEDtEzKZpWt1ms9nqNoOoaZpmf3vdyQ4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIWOmjpIHj42SHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIv4HPXCq6viQRX4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### STRIP_START ###\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(X_train[3].reshape(img_rows,img_cols), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T16:33:17.276747Z",
     "start_time": "2019-05-29T16:33:17.272840Z"
    }
   },
   "source": [
    "## II. Build the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:24:31.385068Z",
     "start_time": "2019-05-30T12:24:31.374005Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. The Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**. The first step is to build a generator. For the generator, we start with an **input noise shape of size 100**. We then create a sequential model to increase the size of the data up to 1024, before reshaping the data back to the input image shape.\n",
    "\n",
    "Each layer will be made of:\n",
    "- A **Dense layer** (sizes 256, 512, 1024 in order)\n",
    "- A **LeakyRelu activation** with alpha = 0.2\n",
    "- A **Batch normalization** (momentum = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:24:32.487781Z",
     "start_time": "2019-05-30T12:24:32.468680Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    # Input Data\n",
    "    noise_shape = (100,)\n",
    "    noise = Input(shape=noise_shape)\n",
    "    \n",
    "    # Create the sequential model\n",
    "    ### STRIP_START ###\n",
    "    model = Sequential()\n",
    "    ### STRIP_END ###\n",
    "\n",
    "    # Build the first layer\n",
    "    ### STRIP_START ###\n",
    "    model.add(Dense(256, input_shape=noise_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    ### STRIP_END ###\n",
    "    \n",
    "    # Second layer\n",
    "    ### STRIP_START ###\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    ### STRIP_END ###\n",
    "    \n",
    "    # Third layer\n",
    "    ### STRIP_START ###\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    ### STRIP_END ###\n",
    "    \n",
    "    # Flatten and reshape\n",
    "    ### STRIP_START ###\n",
    "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "    model.add(Reshape(img_shape))\n",
    "    ### STRIP_END ###\n",
    "\n",
    "    # Get model summary\n",
    "    ### STRIP_START ###\n",
    "    img = model(noise)\n",
    "    model.summary()\n",
    "    ### STRIP_END ###\n",
    "    \n",
    "    return Model(noise, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**. Compile the Generator and add an Adam optimizer as advised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:24:34.447550Z",
     "start_time": "2019-05-30T12:24:33.386567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### STRIP_START ###\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "generator = build_generator()\n",
    "generator.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer=optimizer)\n",
    "### STRIP_END###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. The Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3**. Build the discriminator. It takes an input that has the shape of the image. The steps are the following :\n",
    "- Declare the **Sequential** model\n",
    "- **Flatten** the images (with input shape = image shape)\n",
    "- Add a **Dense layer** of 512 and a **Leaky Relu** (0.2)\n",
    "- Add a **Dense layer** of 256 and a **Leaky Relu** (0.2)\n",
    "- Add a **Dense layer** of size 1. What activation function would you use ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:24:34.899788Z",
     "start_time": "2019-05-30T12:24:34.885304Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    \n",
    "    img = Input(shape=img_shape)\n",
    "\n",
    "    # Create the sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Flatten the images taken as inputs\n",
    "    ### STRIP_START ###\n",
    "    model.add(Flatten(input_shape=img_shape))\n",
    "    ### STRIP_END ###\n",
    "    \n",
    "    # First layer\n",
    "    ### STRIP_START ###\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    ### STRIP_END ###\n",
    "    \n",
    "    # Second layer\n",
    "    ### STRIP_START ###\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    ### STRIP_END ###\n",
    "    \n",
    "    # Last layer, return either 0 or 1\n",
    "    ### STRIP_START ###\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    ### STRIP_END ###\n",
    "    \n",
    "    # Get model summary\n",
    "    ### STRIP_START ###\n",
    "    validity = model(img)\n",
    "    model.summary()\n",
    "    ### STRIP_END ###\n",
    "    \n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**. Now compile the discriminator. (Observe the metric we are using)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:24:35.562291Z",
     "start_time": "2019-05-30T12:24:35.358591Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### STRIP_START ###\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(\n",
    "            loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Build the GAN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5**. Now it is time to build **the GAN model**. This is done in 4 major steps :\n",
    "- Declare the input\n",
    "- Set the image as the result of the generator of the input\n",
    "- Set the output as the result of the discriminator of the generated image\n",
    "- Define and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:24:37.952337Z",
     "start_time": "2019-05-30T12:24:37.220696Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Declare input of size (100, )\n",
    "### STRIP_START ###\n",
    "z = Input(shape=(100,))\n",
    "### STRIP_END ###\n",
    "\n",
    "# 2. Define the generated image from the input\n",
    "# Hint : Use the generator model compiled above\n",
    "### STRIP_START ###\n",
    "img = generator(z)\n",
    "### STRIP_END ###\n",
    "\n",
    "# 3. Define the output from the image\n",
    "# Hint : Use the discriminator model compiled above\n",
    "### STRIP_START ###\n",
    "valid = discriminator(img)\n",
    "### STRIP_END ###\n",
    "\n",
    "# For the combined model, only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# 4.Combined model\n",
    "# Create the model by defining the input and the output\n",
    "### STRIP_START ###\n",
    "combined = Model(z, valid)\n",
    "### STRIP_END ###\n",
    "# Once created, we compile the model\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6**. Print the summary of the new model created. Comment on the shapes at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:24:38.640080Z",
     "start_time": "2019-05-30T12:24:38.626886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 28, 28, 1)         1493520   \n",
      "_________________________________________________________________\n",
      "model_3 (Model)              (None, 1)                 533505    \n",
      "=================================================================\n",
      "Total params: 2,027,025\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 537,089\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### STRIP_START ###\n",
    "combined.summary()\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a function that is used to save generated images once in a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:24:41.361699Z",
     "start_time": "2019-05-30T12:24:41.349843Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_imgs(epoch):\n",
    "    \n",
    "    # Predict from input noise\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "    \n",
    "    # Subplots\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    \n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    " \n",
    "    fig.savefig(\"images_gan/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we set :\n",
    "- the number of epochs the model will train to 15'000\n",
    "- the batch size to 64\n",
    "- the interval at which we save the images to 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:28:50.729562Z",
     "start_time": "2019-05-30T12:28:50.722302Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 15000\n",
    "batch_size = 64\n",
    "save_interval = 1000\n",
    "half_batch = int(batch_size / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is complete. Try to understand the different steps, debug potential errors from your previous code and compile it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the directory to save the images does not exist create it \n",
    "!mkdir images_gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:51:49.635827Z",
     "start_time": "2019-05-30T12:33:29.842168Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "d_loss_hist = []\n",
    "g_loss_hist = []\n",
    "d_acc = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Discriminator\n",
    "    # ---------------------\n",
    "    \n",
    "    # Pick 50% of sample images\n",
    "    idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "    imgs = X_train[idx]\n",
    "\n",
    "    # Generate 50% of new images\n",
    "    noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "    \n",
    "    \n",
    "    # Train discriminator on real images with label 1\n",
    "    d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "    \n",
    "    # Train discriminator on fake images with label 0\n",
    "    d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "    \n",
    "    # Loss of discriminator = Mean of Real and Fake loss\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    \n",
    "    d_loss_hist.append(d_loss[0])\n",
    "    d_acc.append(d_loss[1])\n",
    "    \n",
    "    # ---------------------\n",
    "    #  Train Generator\n",
    "    # ---------------------\n",
    "\n",
    "    # The generator wants the discriminator to label the generated samples as valid (ones)\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "    valid_y = np.array([1] * batch_size)\n",
    "\n",
    "    # Train the generator\n",
    "    g_loss = combined.train_on_batch(noise, valid_y)\n",
    "    g_loss_hist.append(g_loss)\n",
    "    \n",
    "    # Print the progress\n",
    "    print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "        \n",
    "    if epoch % save_interval == 0:\n",
    "        save_imgs(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Create new samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7**. We now have all the elements required to generate new samples. What are according to you :\n",
    "- the steps to generate new samples ?\n",
    "- the part of the network we re-use ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8**. You are now asked to generate and visualize new samples from the steps you defined above. Pay attention when plotting generated images to :\n",
    "- rescale the images between 0 and 1 (as done previously)\n",
    "- reshape the generated image to 28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-02T18:25:42.255461Z",
     "start_time": "2019-06-02T18:25:41.959038Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### STRIP_START ###\n",
    "noise = np.random.normal(0, 1, (1, 100))\n",
    "gen_imgs = generator.predict(noise)\n",
    "gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "plt.imshow(gen_imgs.reshape(28,28), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "### STRIP_END ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
