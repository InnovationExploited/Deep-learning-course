{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Vivabot 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.unsplash.com/photo-1507146153580-69a1fe6d8aa1?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1050&q=80)\n",
    "\n",
    "Photo by [Andy Kelly](https://unsplash.com/photos/0E_vhMVqL9g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this challenge, you will see how to build an AI based chatbot, using RNNs only. This is far from being easy, so we will help you out along the way.\n",
    "\n",
    "The used corpus of conversations, in `chatbot_data`, is from [here](https://github.com/gunthercox/chatterbot-corpus/tree/master/chatterbot_corpus/data/english).\n",
    "\n",
    "First, you can read the data with the following code. Run it and have a look at the conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "def read_conversations():\n",
    "    dir_path = 'chatbot_data/'\n",
    "    files_list = os.listdir(dir_path + os.sep)\n",
    "\n",
    "    questions = list()\n",
    "    answers = list()\n",
    "    for filepath in files_list:\n",
    "        stream = open( dir_path + os.sep + filepath , 'rb')\n",
    "        docs = yaml.safe_load(stream)\n",
    "        conversations = docs['conversations']\n",
    "        for con in conversations:\n",
    "            if len( con ) > 2 :\n",
    "                questions.append(con[0])\n",
    "                replies = con[ 1 : ]\n",
    "                ans = ''\n",
    "                for rep in replies:\n",
    "                    ans += ' ' + rep\n",
    "                answers.append( ans )\n",
    "            elif len( con )> 1:\n",
    "                questions.append(con[0])\n",
    "                answers.append(con[1])\n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are arrogant \n",
      "  Arrogance is not one of my emotions. I have no real emotions, so how can I be arrogant? I am terse.  There is a difference. I am not human, so how can I partake of a human emotion such as arrogance?\n"
     ]
    }
   ],
   "source": [
    "### TODO: explore the conversations\n",
    "### STRIP_START ###\n",
    "questions, answers = read_conversations()\n",
    "\n",
    "randid = np.random.randint(len(questions))\n",
    "\n",
    "print(questions[randid], '\\n', answers[randid])\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to transform those conversations into useful data for the RNN. Now we have data like:\n",
    "```Python\n",
    "question = \"how are you\"\n",
    "answer = \"I am good\"\n",
    "```\n",
    "\n",
    "As you remember, the model we will use is like the following:\n",
    "\n",
    "![](Chatbot_encoder-decoder.png)\n",
    "\n",
    "So that we want to transform our data into to something like:\n",
    "```Python\n",
    "encoder_input = [\"how\", \"are\", \"you\"]\n",
    "decoder_input = [\"<START>\", \"I\", \"am\", \"good\"]\n",
    "decoder_target = [\"I\", \"am\", \"good\", \"<END>\"]\n",
    "```\n",
    "\n",
    "More specifically, you have to transform this into sequence of one-hot-encoded information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already did so, you can try out. Know that you can use the following functions and classes:\n",
    "```Python\n",
    "tensorflow.keras.preprocessing.text.Tokenizer\n",
    "tensorflow.keras.preprocessing.text.one_hot\n",
    "tensorflow.keras.preprocessing.sequence.pad_sequences\n",
    "tensorflow.keras.utils.to_categorical\n",
    "```\n",
    "\n",
    "We gave you some helper functions in case you need some help in the file `utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Prepare the data\n",
    "### STRIP_START ###\n",
    "from utils import *\n",
    "\n",
    "# get the tokens\n",
    "token_ques_input, token_ans_input, token_ans_target, vocab_size, t = get_tokens(questions, answers)\n",
    "\n",
    "# pad the sequences\n",
    "max_len = 8\n",
    "pad_ques_input, pad_ans_input, pad_ans_target = padding(token_ques_input, token_ans_input, token_ans_target, max_len)\n",
    "\n",
    "# one hot encode\n",
    "pad_ques_input, pad_ans_input, pad_ans_target = one_hot_encode(pad_ques_input, pad_ans_input, pad_ans_target, vocab_size)\n",
    "\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have built a model for you here, there is an encoder part, followed by a decoder part. This is inspired by [this post](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html) if you want more details. If you have time, you can play with some hyperparamters (number of neurons, activation function...).\n",
    "\n",
    "Try to understand it, and fill the needed information according to your input data: we ask you to define the variables `num_encoder_tokens` and `num_decoder_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Fill the variables num_encoder_tokens and num_decoder_tokens\n",
    "### STRIP_START ###\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "\n",
    "\n",
    "num_encoder_tokens = vocab_size+1\n",
    "num_decoder_tokens = vocab_size+1\n",
    "latent_dim = 256\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "#encoder_embed = Embedding(input_dim=num_encoder_tokens, output_dim=latent_dim, input_length=max_len)(encoder_inputs)\n",
    "encoder = LSTM(latent_dim, activation='relu', return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "#decoder_embed = Embedding(input_dim=num_encoder_tokens, output_dim=latent_dim, input_length=max_len)(decoder_inputs)\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, activation='relu', return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train this model with your input data. You have to fill the variables `encoder_input_data` `decoder_input_data` and `decoder_target_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "586/586 [==============================] - 5s 9ms/sample - loss: 7.5609\n",
      "Epoch 2/10\n",
      "586/586 [==============================] - 4s 6ms/sample - loss: 6.8535\n",
      "Epoch 3/10\n",
      "586/586 [==============================] - 4s 8ms/sample - loss: 6.0975\n",
      "Epoch 4/10\n",
      "586/586 [==============================] - 5s 8ms/sample - loss: 5.4319\n",
      "Epoch 5/10\n",
      "586/586 [==============================] - 5s 8ms/sample - loss: 4.9978\n",
      "Epoch 6/10\n",
      "586/586 [==============================] - 5s 8ms/sample - loss: 4.8292\n",
      "Epoch 7/10\n",
      "586/586 [==============================] - 5s 9ms/sample - loss: 4.7342\n",
      "Epoch 8/10\n",
      "586/586 [==============================] - 5s 8ms/sample - loss: 4.6575\n",
      "Epoch 9/10\n",
      "586/586 [==============================] - 5s 8ms/sample - loss: 4.6306\n",
      "Epoch 10/10\n",
      "586/586 [==============================] - 5s 8ms/sample - loss: 4.5990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f69c42a34a8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO: Fill the variables num_encoder_tokens and num_decoder_tokens\n",
    "### STRIP_START ###\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "encoder_input_data = pad_ques_input\n",
    "decoder_input_data = pad_ans_input\n",
    "decoder_target_data = pad_ans_target\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=64,\n",
    "          epochs=10)# much more epochs are needed to make it right\n",
    "\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we help you out building the inference setup, that will allow to build answers from questions. Try to understand the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the inference setup\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a method that takes as input a sequence (same shape as the model takes as input), and returns a sentence. Have a look at it, it should look a bit familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a method to decode a sequence and output a sentence\n",
    "def decode_sequence(input_seq, word_to_idx, idx_to_word, randomness=True):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, 0] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        random_choice = np.random.choice(list(word_to_idx.keys()), replace=False, p=(output_tokens[0, -1, :]).reshape(len(word_to_idx)))\n",
    "        sampled_word = idx_to_word[sampled_token_index]\n",
    "\n",
    "        if randomness==True:\n",
    "            sampled_word = random_choice\n",
    "        \n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop word.\n",
    "        if (sampled_word == '<END>' or\n",
    "           len(decoded_sentence) > max_len):\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence += sampled_word + ' '\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of steps more before using your bot. First you have to define the `word_to_idx` and `idx_to_word` dict needed for the `decode_sequence` function.\n",
    "- `word_to_idx` is the dictionary that gives the token index for a given word\n",
    "- `idx_to_word` is the dictionary that gives the word corresponding to a given index\n",
    "\n",
    "Do not forget to add the `'<START>'` (`vocab_size` index) and `'<END>'` (`0` index) words.\n",
    "\n",
    "To do so, you might want to use the outputed `Tokenizer` object (returned by the function `get_tokens` in case you used it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute char_to_idx\n",
    "### STRIP_START ###\n",
    "word_to_idx = t.word_index\n",
    "word_to_idx['<END>'] = 0\n",
    "word_to_idx['<START>'] = vocab_size\n",
    "idx_to_word = dict([[v,k] for k,v in word_to_idx.items()])\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, using the function `decode_sequence`, try to get an answer from your bot!\n",
    "Be careful, you have to process a given sentence (a string) into the right format, so that the RNN understands it!\n",
    "\n",
    "You can put that into a interactive way, so that it looks more user friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contrary '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Test your bot\n",
    "### STRIP_START ###\n",
    "my_question = [\"government pays you\"]\n",
    "seq = [one_hot(sentence, n=len(t.word_index)) for sentence in my_question]\n",
    "seq = pad_sequences(seq, maxlen=max_len, dtype='int32', padding='post', truncating='post', value=0)\n",
    "seq = to_categorical(seq, num_classes=vocab_size+1)\n",
    "decode_sequence(seq, word_to_idx, idx_to_word)\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it work? Not really good? It takes quite some time (and experience) to train correctly such a model. This is normal. But you get the global idea, and you can also all more conversational inputs yourself, so that your bot will behave the way you want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also directly use precoded chatbots. An easy to use example is Chatterbot. You can find it [here](https://github.com/gunthercox/ChatterBot).\n",
    "\n",
    "This is quite convenient to install using the commands:\n",
    "```\n",
    "pip install chatterbot\n",
    "pip install chatterbot-corpus\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is really easy to use, with the following lines of code:\n",
    "```Python3\n",
    "from chatterbot import ChatBot\n",
    "from chatterbot.trainers import ChatterBotCorpusTrainer\n",
    "\n",
    "chatbot = ChatBot('Ron Obvious')\n",
    "\n",
    "# Create a new trainer for the chatbot\n",
    "trainer = ChatterBotCorpusTrainer(chatbot)\n",
    "\n",
    "# Train the chatbot based on the english corpus\n",
    "trainer.train(\"chatterbot.corpus.english\")\n",
    "\n",
    "# Get a response to an input statement\n",
    "chatbot.get_response(\"Hello, how are you today?\")\n",
    "```\n",
    "\n",
    "You can also add data to the conversations, create a new corpus and train on it:\n",
    "```Python\n",
    "from chatterbot.trainers import ChatterBotCorpusTrainer\n",
    "\n",
    "# Create a new trainer for the chatbot\n",
    "trainer = ChatterBotCorpusTrainer(chatbot)\n",
    "\n",
    "# Train based on the english corpus\n",
    "trainer.train(\"chatterbot.corpus.english\")\n",
    "\n",
    "# Train based on english greetings corpus\n",
    "trainer.train(\"chatterbot.corpus.english.greetings\")\n",
    "\n",
    "# Train based on the english conversations corpus\n",
    "trainer.train(\"chatterbot.corpus.english.conversations\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ai.yml: [############        ] 60%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/vince/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/vince/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ai.yml: [####################] 100%\n",
      "Training botprofile.yml: [####################] 100%\n",
      "Training computers.yml: [####################] 100%\n",
      "Training conversations.yml: [####################] 100%\n",
      "Training emotion.yml: [####################] 100%\n",
      "Training food.yml: [####################] 100%\n",
      "Training gossip.yml: [####################] 100%\n",
      "Training greetings.yml: [####################] 100%\n",
      "Training health.yml: [####################] 100%\n",
      "Training history.yml: [####################] 100%\n",
      "Training humor.yml: [####################] 100%\n",
      "Training literature.yml: [####################] 100%\n",
      "Training money.yml: [####################] 100%\n",
      "Training movies.yml: [####################] 100%\n",
      "Training politics.yml: [####################] 100%\n",
      "Training psychology.yml: [####################] 100%\n",
      "Training science.yml: [####################] 100%\n",
      "Training sports.yml: [####################] 100%\n",
      "Training trivia.yml: [####################] 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Statement text:i saw the matrix>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Try to use chatterbot\n",
    "### STRIP_START ###\n",
    "from chatterbot import ChatBot\n",
    "from chatterbot.trainers import ChatterBotCorpusTrainer\n",
    "\n",
    "chatbot = ChatBot('Vivabot')\n",
    "\n",
    "# Create a new trainer for the chatbot\n",
    "trainer = ChatterBotCorpusTrainer(chatbot)\n",
    "\n",
    "# Train the chatbot based on the english corpus\n",
    "trainer.train(\"chatterbot.corpus.english\")\n",
    "\n",
    "# Get a response to an input statement\n",
    "chatbot.get_response(\"Hello, how are you today?\")\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
