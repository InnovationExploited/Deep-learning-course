{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04-Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.unsplash.com/photo-1478796415026-3c85ee65975e?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1050&q=80)\n",
    "Photo by [Gaelle Marcel](https://unsplash.com/photos/gIj7RJPAkJA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will implement your own forward propagation of a neural network, using Python and numpy only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider again our example of neural network in the lectures:\n",
    "![](../../../00-Lectures/images/MLP_with_activations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by defining a numpy array `X` with 3 random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5488135 , 0.71518937, 0.60276338])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: define X\n",
    "### STRIP_START ###\n",
    "import numpy as np\n",
    "\n",
    "X=np.random.rand(3)\n",
    "X\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now redefine the sigmoid function, which will be our activation function in our neural network.\n",
    "\n",
    "Reminder:\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1 + e^{- x}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the sigmoid function as g(x)\n",
    "### STRIP_START ###\n",
    "def g(x):\n",
    "    return 1./(1+np.exp(-x))\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see on the plot, $a^{[1]}_1$ is computed with the values $x_1$, $x_2$ and $x_3$ as well as the associated weights $W^{[1]}_{1}$ and $b^{[1]}_{1}$:\n",
    "\n",
    "$$\n",
    "a^{[1]}_{1} = g(W^{[1]}_{1} \\times X + b^{[1]}_{1})\n",
    "$$\n",
    "\n",
    "Begin by defining randomly $W^{[1]}_{1}$ (which is a numpy array of three values) and $b^{[1]}_{1}$ and then compute $a^{[1]}_{1}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69207739])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Compute the activation of the first unit of the first layer a11\n",
    "### STRIP_START ###\n",
    "W11 = np.random.rand(3)\n",
    "b11 = np.random.rand(1)\n",
    "\n",
    "a11 = g(np.dot(X, np.transpose(W11))+b11)\n",
    "a11\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for the two other units of the first layer: compute $a^{[1]}_2$, $a^{[1]}_3$ and $a^{[1]}_4$ as `a12`, `a13` and `a14`.\n",
    "\n",
    "Reminder:\n",
    "\n",
    "$$\n",
    "a^{[1]}_{2} = g(W^{[1]}_{2} \\times X + b^{[1]}_{2})\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{[1]}_{3} = g(W^{[1]}_{3} \\times X + b^{[1]}_{3})\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{[1]}_{4} = g(W^{[1]}_{4} \\times X + b^{[1]}_{4})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute a12, a13 and a14\n",
    "### STRIP_START ###\n",
    "W12 = np.random.rand(3)\n",
    "b12 = np.random.rand(1)\n",
    "\n",
    "W13 = np.random.rand(3)\n",
    "b13 = np.random.rand(1)\n",
    "\n",
    "W14 = np.random.rand(3)\n",
    "b14 = np.random.rand(1)\n",
    "\n",
    "a12 = g(np.dot(X, np.transpose(W12))+b12)\n",
    "a13 = g(np.dot(X, np.transpose(W13))+b13)\n",
    "a14 = g(np.dot(X, np.transpose(W14))+b14)\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to transform our values $a^{[1]}_i$ (saved into `a11`, `a12`, `a13` and `a14` into a single vector of 4 values `a1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69207739, 0.84369753, 0.88187849, 0.76261839])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: compute a1\n",
    "### STRIP_START ###\n",
    "a1 = np.concatenate([a11, a12, a13, a14])\n",
    "a1\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer is computed, now let's continue, we want to compute the values of the second layer, using the following formulas, still defining random values for weights and bias:\n",
    "\n",
    "$$\n",
    "a^{[2]}_{1} = g( W^{[2]}_{1} \\times a^{[1]} + b^{[2]}_{1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{[2]}_{2} = g(W^{[2]}_{2} \\times a^{[1]} + b^{[2]}_{2})\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{[2]}_{3} = g(W^{[2]}_{3} \\times a^{[1]} + b^{[2]}_{3})\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{[2]}_{4} = g(W^{[2]}_{4} \\times a^{[1]} + b^{[2]}_{4})\n",
    "$$\n",
    "\n",
    "Be careful, now the weights $W^{[2]}_i$ might not have the same dimension..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute a21, a22, a23, a24\n",
    "### STRIP_START ###\n",
    "W21 = np.random.rand(4)\n",
    "b21 = np.random.rand(1)\n",
    "\n",
    "W22 = np.random.rand(4)\n",
    "b22 = np.random.rand(1)\n",
    "\n",
    "W23 = np.random.rand(4)\n",
    "b23 = np.random.rand(1)\n",
    "\n",
    "W24 = np.random.rand(4)\n",
    "b24 = np.random.rand(1)\n",
    "\n",
    "a21 = g(np.dot(a1, np.transpose(W21))+b21)\n",
    "a22 = g(np.dot(a1, np.transpose(W22))+b22)\n",
    "a23 = g(np.dot(a1, np.transpose(W23))+b23)\n",
    "a24 = g(np.dot(a1, np.transpose(W24))+b24)\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, compute the vector `a2`, concatenation of `a21`, `a22`, `a23` and `a24`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.74922896, 0.86458958, 0.90727622, 0.84250911])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: compute a2\n",
    "### STRIP_START ###\n",
    "a2 = np.concatenate([a21, a22, a23, a24])\n",
    "a2\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, compute the output value `a3` using the following formula:\n",
    "\n",
    "$$\n",
    "a^{[3]}_{1} = g(W^{[3]}_{1} \\times a^{[2]} + b^{[3]}_{1})\n",
    "$$\n",
    "\n",
    "Again with random weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82879239]\n"
     ]
    }
   ],
   "source": [
    "# TODO: compute a3\n",
    "### STRIP_START ###\n",
    "W31 = np.random.rand(4)\n",
    "b31 = np.random.rand(1)\n",
    "\n",
    "a3 = g(np.dot(a2, np.transpose(W31))+b31)\n",
    "print(a3)\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have built your own neural network, impressive, right?\n",
    "\n",
    "You now know how a neural network can compute a value (for regression or classification) just by having units and layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is optional and a bit more complicated.\n",
    "\n",
    "You might have noticed that we made a lot of computations that could be vectorized. Meaning, instead of computing separately `a11`, `a12`, `a13`, `a14`, we could have compute directly `a1`. It works for other layers too. We will do it that way now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep our input vector `X`, and define a weight matrix `W1` and a bias vector `b1` randomly. And then, having those three variables, compute `a1` in just one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72901137, 0.82103318, 0.82225048, 0.8266251 ])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: compute a1 in one line\n",
    "### STRIP_START ###\n",
    "W1 = np.random.rand(3, 4)\n",
    "b1 = np.random.rand(4)\n",
    "\n",
    "a1 = g(np.dot(X, W1) + b1)\n",
    "a1\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets easier, right? Now that you got it, compute `a2` and finally `a3` in just a couple of lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94148338])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: compute a2 and a3\n",
    "### STRIP_START ###\n",
    "W2 = np.random.rand(4, 4)\n",
    "b2 = np.random.rand(4)\n",
    "W3 = np.random.rand(4)\n",
    "b3 = np.random.rand(1)\n",
    "\n",
    "a2 = g(np.dot(a1, W2) + b2)\n",
    "a3 = g(np.dot(a2, W3) + b3)\n",
    "\n",
    "a3\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process is called vectorization, and helps a lot in computing matrix calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you still have time, you can try to define a function that takes as parameters an input vector `X`, the number of layers `L` and the units per layer `units` and returns the output of the associated neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have more time, try to generalize what you did: build a function (or a class!) that computes the forward propagation, with the number of input features, layers and units per layers as **parameters** of the function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
