{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-LeNet5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.unsplash.com/photo-1495592528496-a143a67931d6?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1050&q=80)\n",
    "\n",
    "Photo by [Pietro Jeng](https://unsplash.com/photos/sQVXS8HBPPc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will apply the LeNet5 algorithm to the Fashion MNIST dataset and improve your performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first download the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "#X_train = X_train[:10000]\n",
    "#y_train = y_train[:10000]\n",
    "#X_test = X_test[:2000]\n",
    "#y_test = y_test[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you already know, this dataset contains 10 classes:\n",
    "* 0:\tT-shirt/top\n",
    "* 1:\tTrouser\n",
    "* 2:\tPullover\n",
    "* 3:\tDress\n",
    "* 4:\tCoat\n",
    "* 5:\tSandal\n",
    "* 6:\tShirt\n",
    "* 7:\tSneaker\n",
    "* 8:\tBag\n",
    "* 9:\tAnkle boot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can have a look at some images if needed, even if you already know them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAEtZJREFUeJzt3X2MlWV6BvDrYuQbBISBUEAQQrFarLtMtOkaY7Wy+O3GaJe0hia07B+SVGNSLbvJ+kebmmbdjWmbTVilixvWXZOVSlPSoqbGaJONI0H5iqKIAuLMICBfAjLc/WMOm0Hn3Pc47znnPTP39UsIZ8593jnPHL1455z7fZ6HZgYRyWdY2QMQkXIo/CJJKfwiSSn8Ikkp/CJJKfwiSSn8Ikkp/ImR3EPyz8oeh5RD4RdJSuFPiuQvAFwK4D9JHif5dyTvJLmd5BGSr5D8g16P30Py70nuIHmY5L+THFXeTyBFKfxJmdn9AD4CcIeZjQPwHwCeBfAggFYAG9HzD8OIXof9BYBvA5gH4PcB/KChg5aaUvjlvD8H8F9m9qKZfQHgRwBGA/iTXo/5VzPba2aHAPwjgKUljFNqROGX834PwIfnvzCzcwD2ApjR6zF7e93+sHKMDFIKf269p3R+DGD2+S9IEsAsAPt7PWZWr9uXVo6RQUrhz60DwNzK7ecA3EbyJpLDATwM4DSA/+v1+AdIziR5CYDvA/h1Q0crNaXw5/ZPAH5A8giAOwD8JYB/AXCw8vUdZnam1+N/CWATgN0A3gfwD40drtQStZiH9AfJPQD+2sxeKnssUhs684skpfCLJKVf+0WS0plfJKmLGvlkU6ZMsTlz5jTyKQeFo0ePuvXot7Phw4dXrV10kf+fOHpu73sDQEtLi1v//PPPq9bOnTvnHjtixAi3PmnSJLee0Z49e3Dw4EH257GFwk9yCYAnAbQAeMrMHvceP2fOHLS3txd5yiFp06ZNbv3MmTNufebMmVVrUUBeesn/8H7q1KlufeLEiW5969atVWsnT550j509e7Zbv/fee916Rm1tbf1+7IB/7SfZAuDfANwC4AoAS0leMdDvJyKNVeQ9/zUA3jOz3ZULQX4F4K7aDEtE6q1I+Gfgwoke+3DhJBAAAMkVJNtJtnd1dRV4OhGppbp/2m9mq82szczaWltb6/10ItJPRcK/HxfO8pqJC2eAiUgTKxL+NwDMJ3lZZbWX7wLYUJthiUi9DbjVZ2ZnSa4E8D/oafWtMbPtNRtZk/niiy+q1p544gn32M2bN7v1s2fPuvVZs2a59ePHj1etRW3CdevWufVx48a59dtvv92tR61Az1tvveXWN2zwzzXjx4+vWlu+fLl77KJFi9z6UFCoz29mG9Gz1puIDDK6vFckKYVfJCmFXyQphV8kKYVfJCmFXySphs7nb2bR3PJ77rmnam3UKH/LumuvvdatR730w4cPu/XRo0dXrUXz7W+88Ua3Hr0unZ2dbt1bD8Cb6w8ACxYscOvRlGBvLsmqVavcYx966CG3vmTJErc+GOjML5KUwi+SlMIvkpTCL5KUwi+SlMIvkpRafRWPPPKIW/dWkp07d27VGuBPBwb8KbkAMHLkSLd+6NChqjWvDQgAR44ccetRqy/Ss9P3wEStvOh7X3bZZVVr0arGa9ascetq9YnIoKXwiySl8IskpfCLJKXwiySl8IskpfCLJJWmzx/10t9//323fuWVV1atffrpp+6x0ZTd06dPu/WItw13tHR3ZOzYsW49mpbrTemNlvU+ePCgW4941wlE1z9E26Lv2rXLrc+fP9+tNwOd+UWSUvhFklL4RZJS+EWSUvhFklL4RZJS+EWSStPnf+2119x61K8eNqz6v5PHjh1zj43m80f97u7ubrfuLR1+6tQp99gxY8YM+HsDwIgRI9z6xx9/XLX2+uuvu8cuXrzYrR89etStez+7twZCf3zwwQdufTD0+QuFn+QeAMcAdAM4a2ZttRiUiNRfLc78f2pmxS7FEpGG03t+kaSKht8AbCL5JskVfT2A5AqS7STbve2TRKSxiob/OjP7JoBbADxA8vovP8DMVptZm5m1tba2Fnw6EamVQuE3s/2VvzsBrAdwTS0GJSL1N+DwkxxLcvz52wAWA9hWq4GJSH0V+bR/GoD1lbXTLwLwSzP775qMqg62b9/u1idPnuzWi8yLj/r8J06ccOveNQaRaNxRnz7qpUev2yuvvFK1tnv3bvdYb50CALj++q+8y7yA97NH8/mjNRjeffddtx5do9AMBhx+M9sN4I9qOBYRaSC1+kSSUvhFklL4RZJS+EWSUvhFkkozpTdq9Y0fP96te1N+o2mv0VbT0dLdEyZMcOve8tgtLS3usdEW3FErMFpe22tjeltoA0DRy8G95dqjadLRFt47duwY0Jiaic78Ikkp/CJJKfwiSSn8Ikkp/CJJKfwiSSn8Ikml6fN3dna69YULF7p1r58dTZuNrgOIppcWEU0HjqbNRqJlx6dPn161tmnTJvfYlStXDmhM53lTqaMtuKPrH6Il0aPvX5kKXyqd+UWSUvhFklL4RZJS+EWSUvhFklL4RZJS+EWSGjJ9/n379rn1aP521Jf1tuGO+vTRTkXR2KL5/tF1BJ5oa/JIdI3DsmXLqtZuu+0299izZ8+69b1797r1jo6OqrW5c+cWeu7Dhw+79a1bt7r1q666yq03gs78Ikkp/CJJKfwiSSn8Ikkp/CJJKfwiSSn8IkkNmT7/888/79ajXnw0P9s7PurDR3Pmo+f21uUH/LnnUR8+2j48uv4hGttHH31UtRaNLfre0Tba3l4N3joDQDy2iy++2K1v2LDBrQ+KPj/JNSQ7SW7rdd8lJF8kuavyt7/DgYg0nf782v9zAEu+dN+jAF42s/kAXq58LSKDSBh+M3sVwKEv3X0XgLWV22sB3F3jcYlInQ30A79pZnagcvsTANOqPZDkCpLtJNuL7r0mIrVT+NN+6/lEqOqnQma22szazKwtmuAiIo0z0PB3kJwOAJW//aVxRaTpDDT8GwCcn6u5DMALtRmOiDRK2Ocn+SyAGwBMIbkPwA8BPA7gOZLLAXwI4L56DrI/Fi9e7NajXvvOnTvd+rZt29y6Z968eW49WsO9yHz9SEtLi1uPxhatbz9y5MivPabzojn1UZ//nXfeqVqbNMnvTk+dOtWtL1iwwK3ffPPNbr0ZhOE3s6VVSjfVeCwi0kC6vFckKYVfJCmFXyQphV8kKYVfJKkhM6X38ssvL1QvYuPGjW59/fr1bv3SSy+t5XAuELXyolZd1OqLvn8R0ZTeaOnudevWVa3deeed7rH1/Lmahc78Ikkp/CJJKfwiSSn8Ikkp/CJJKfwiSSn8IkkNmT5/tMR0JOpnexYtWuTWn3rqKbc+e/Zstx5N6fWmvg4bVuzf96jfHb3u3tiiKbvRz+1tmw74W6Nn6ONHdOYXSUrhF0lK4RdJSuEXSUrhF0lK4RdJSuEXSWrI9PmL9OmBuF/tff+oZzxhwoQBjem87u5ut+4tSx69LtEW3dHxRa6vGDFihFuP1hqI6keOHPnaYzoves2HwnUCOvOLJKXwiySl8IskpfCLJKXwiySl8IskpfCLJDVk+vxFFblOIOrjR73wem6THfWro155NOe+SL87el2i6wCidf2LKHrdyGAQnvlJriHZSXJbr/seI7mf5JbKn1vrO0wRqbX+/Nr/cwBL+rj/J2Z2deWPv2WNiDSdMPxm9iqAQw0Yi4g0UJEP/FaSfLvytmBStQeRXEGynWR7V1dXgacTkVoaaPh/CmAegKsBHADwRLUHmtlqM2szszZvQUURaawBhd/MOsys28zOAfgZgGtqOywRqbcBhZ/k9F5ffgfAtmqPFZHmFPb5ST4L4AYAU0juA/BDADeQvBqAAdgD4Ht1HGNDFJnPH/Wbo/Xno158tPa+Nyc/6sNHz13vdRI80TUG0TUK9bwOYCgIw29mS/u4++k6jEVEGkiX94okpfCLJKXwiySl8IskpfCLJKUpvQ0wcuRItz569Gi3Hi2v7YlaeVE7LJpWW0/ekuRA8e3HPUW3fB8MdOYXSUrhF0lK4RdJSuEXSUrhF0lK4RdJSuEXSUp9/gY4c+aMWz958qRbHzNmTC2Hc4GiW3BHdW9abnQNQTQdObpGIbpOIDud+UWSUvhFklL4RZJS+EWSUvhFklL4RZJS+EWSUiO0AaL5/FG/O5rPX2R57egahDKXv46W5i66/bhHW3SLyJCl8IskpfCLJKXwiySl8IskpfCLJKXwiyTVny26ZwF4BsA09GzJvdrMniR5CYBfA5iDnm267zOzw/Ub6tBVZHtwwO9nF113P5pTH/Hm80d9+uj6iGi+ftGxD3X9OfOfBfCwmV0B4I8BPEDyCgCPAnjZzOYDeLnytYgMEmH4zeyAmW2u3D4GYCeAGQDuArC28rC1AO6u1yBFpPa+1nt+knMAfAPAbwFMM7MDldIn6HlbICKDRL/DT3IcgN8AeNDMjvauWc+b1j7fuJJcQbKdZHtXV1ehwYpI7fQr/CSHoyf468zs+crdHSSnV+rTAXT2dayZrTazNjNra21trcWYRaQGwvCz56PmpwHsNLMf9yptALCscnsZgBdqPzwRqZf+TOn9FoD7AWwluaVy3yoAjwN4juRyAB8CuK8+Q2x+0dTRaGnuqB0XHe9Nu/VabQBw+vRptx6NrcjU16jVF01lLtoizS4Mv5m9BqDaq3hTbYcjIo2iK/xEklL4RZJS+EWSUvhFklL4RZJS+EWS0tLdNTBsmP9vaDQ1Nerjjxo1yq17vfyoVx712iNFtsGOro+I+vTR6x5dB5CdzvwiSSn8Ikkp/CJJKfwiSSn8Ikkp/CJJKfwiSanP3wBF55UXWQ9g9OjRhZ771KlTbj362YqsNRDVo2sYxKczv0hSCr9IUgq/SFIKv0hSCr9IUgq/SFIKv0hS6vPXQDQv/cyZM249mnfu9coBv98d9cqj547qUa/9xIkTVWvRzxWtgxApch1AhjX/deYXSUrhF0lK4RdJSuEXSUrhF0lK4RdJSuEXSSrs85OcBeAZANMAGIDVZvYkyccA/A2ArspDV5nZxnoNtJkV3Se+6HUA3rr+0TUI0VoB0XUCY8aMcetjx46tWov68NGeAN46BgDw2WefuXVPhj5/fy7yOQvgYTPbTHI8gDdJvlip/cTMflS/4YlIvYThN7MDAA5Ubh8juRPAjHoPTETq62u95yc5B8A3APy2ctdKkm+TXENyUpVjVpBsJ9ne1dXV10NEpAT9Dj/JcQB+A+BBMzsK4KcA5gG4Gj2/GTzR13FmttrM2sysrbW1tQZDFpFa6Ff4SQ5HT/DXmdnzAGBmHWbWbWbnAPwMwDX1G6aI1FoYfvZ87Pk0gJ1m9uNe90/v9bDvANhW++GJSL3059P+bwG4H8BWklsq960CsJTk1ehp/+0B8L26jHAQOHz4sFvv6Ohw6zNm+J+fRi2xY8eOVa1FbcKoXRbVI14bM2ojnj592q1Hy4ofPHjQrWfXn0/7XwPQV9MzZU9fZKjQFX4iSSn8Ikkp/CJJKfwiSSn8Ikkp/CJJaenuGrj44ovd+sKFC936tGnT3HrU5+/u7q5ai6amektr9+e5W1paBlwfN26ce+zkyZPd+sSJE9169LpnpzO/SFIKv0hSCr9IUgq/SFIKv0hSCr9IUgq/SFKM5nvX9MnILgAf9rprCoBmnXTdrGNr1nEBGttA1XJss82sX+vlNTT8X3lyst3M2kobgKNZx9as4wI0toEqa2z6tV8kKYVfJKmyw7+65Of3NOvYmnVcgMY2UKWMrdT3/CJSnrLP/CJSEoVfJKlSwk9yCcl3SL5H8tEyxlANyT0kt5LcQrK95LGsIdlJcluv+y4h+SLJXZW/+9wjsaSxPUZyf+W120Ly1pLGNovk/5LcQXI7yb+t3F/qa+eMq5TXreHv+Um2AHgXwM0A9gF4A8BSM9vR0IFUQXIPgDYzK/2CEJLXAzgO4Bkz+8PKff8M4JCZPV75h3OSmT3SJGN7DMDxsrdtr+wmNb33tvIA7gbwVyjxtXPGdR9KeN3KOPNfA+A9M9ttZmcA/ArAXSWMo+mZ2asADn3p7rsArK3cXoue/3karsrYmoKZHTCzzZXbxwCc31a+1NfOGVcpygj/DAB7e329DyW+AH0wAJtIvklyRdmD6cM0MztQuf0JAH8NsMYLt21vpC9tK980r91AtruvNX3g91XXmdk3AdwC4IHKr7dNyXreszVTr7Zf27Y3Sh/byv9Oma/dQLe7r7Uywr8fwKxeX8+s3NcUzGx/5e9OAOvRfFuPd5zfIbnyd2fJ4/mdZtq2va9t5dEEr10zbXdfRvjfADCf5GUkRwD4LoANJYzjK0iOrXwQA5JjASxG8209vgHAssrtZQBeKHEsF2iWbdurbSuPkl+7ptvu3swa/gfArej5xP99AN8vYwxVxjUXwFuVP9vLHhuAZ9Hza+AX6PlsZDmAyQBeBrALwEsALmmisf0CwFYAb6MnaNNLGtt16PmV/m0AWyp/bi37tXPGVcrrpst7RZLSB34iSSn8Ikkp/CJJKfwiSSn8Ikkp/CJJKfwiSf0/sb1JnlWzjO8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Explore the data, display some input images\n",
    "### STRIP_START ###\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "label_class = ['top', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "\n",
    "idx = np.random.randint(X_train.shape[0])\n",
    "\n",
    "plt.imshow(X_train[idx], cmap='gray_r')\n",
    "plt.title(label_class[y_train[idx]])\n",
    "plt.show()\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the data preparation and preprocessing: scale and reshape the data, put the labels to the good shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make the data preparation\n",
    "### STRIP_START ###\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=10)\n",
    "y_test_cat = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "X_train_norm = X_train/255.\n",
    "X_test_norm = X_test/255.\n",
    "\n",
    "#X_train_norm = X_train_norm.reshape(X_train_norm.shape[0], np.prod(X_train_norm.shape[1:]))\n",
    "#X_test_norm = X_test_norm.reshape(X_test_norm.shape[0], np.prod(X_test_norm.shape[1:]))\n",
    "\n",
    "\n",
    "X_train_norm = X_train_norm.reshape(X_train_norm.shape[0], 28, 28, 1)\n",
    "X_test_norm = X_test_norm.reshape(X_test_norm.shape[0], 28, 28, 1)\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build the LeNet5 architecture. You can reuse the one of the course, or try to build it by yourself.\n",
    "\n",
    "The architecture is the following:\n",
    "\n",
    "![](../../../00-Lectures/images/Lenet5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build your model\n",
    "### STRIP_START ###\n",
    "from keras.models import Sequential\n",
    "from keras.layers import MaxPooling2D, Conv2D, Flatten, Dense\n",
    "\n",
    "\n",
    "def lenet5():\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # Layer C1\n",
    "    model.add(Conv2D(filters=6, name='C1', kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "    # Layer S2\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), name='S2'))\n",
    "    # Layer C3\n",
    "    model.add(Conv2D(filters=16, name='C3', kernel_size=(3, 3), activation='relu'))\n",
    "    # Layer S4\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), name='S4'))\n",
    "    # Before going into layer C5, we flatten our units\n",
    "    model.add(Flatten())\n",
    "    # Layer C5\n",
    "    model.add(Dense(units=120, activation='relu', name='C5'))\n",
    "    # Layer F6\n",
    "    model.add(Dense(units=84, activation='relu', name='F6'))\n",
    "    # Output layer\n",
    "    model.add(Dense(units=10, activation = 'softmax'))\n",
    "    \n",
    "    return model\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compile and fit your model on your training data. Since this is a multiclass classification, the loss is not `binary_crossentropy` anymore, but `categorical_crossentropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 0.6135 - val_loss: 0.4591\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 15s 258us/step - loss: 0.4176 - val_loss: 0.4287\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 16s 260us/step - loss: 0.3656 - val_loss: 0.3716\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 15s 258us/step - loss: 0.3361 - val_loss: 0.3594\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 16s 260us/step - loss: 0.3117 - val_loss: 0.3388\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 16s 259us/step - loss: 0.2904 - val_loss: 0.3115\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 16s 260us/step - loss: 0.2763 - val_loss: 0.3114\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 16s 260us/step - loss: 0.2594 - val_loss: 0.2937\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 16s 262us/step - loss: 0.2473 - val_loss: 0.2907\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 16s 273us/step - loss: 0.2352 - val_loss: 0.2838\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 16s 273us/step - loss: 0.2244 - val_loss: 0.2912\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 17s 288us/step - loss: 0.2135 - val_loss: 0.2960\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 16s 271us/step - loss: 0.2053 - val_loss: 0.3075\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 17s 291us/step - loss: 0.1947 - val_loss: 0.2909\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 16s 269us/step - loss: 0.1867 - val_loss: 0.2949\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 16s 269us/step - loss: 0.1795 - val_loss: 0.2833\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 16s 269us/step - loss: 0.1706 - val_loss: 0.2950\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 16s 268us/step - loss: 0.1620 - val_loss: 0.2922\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 16s 270us/step - loss: 0.1561 - val_loss: 0.3108\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 16s 269us/step - loss: 0.1513 - val_loss: 0.3294\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 16s 268us/step - loss: 0.1422 - val_loss: 0.3231\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 17s 279us/step - loss: 0.1360 - val_loss: 0.3319\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 16s 271us/step - loss: 0.1308 - val_loss: 0.3236\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 16s 270us/step - loss: 0.1237 - val_loss: 0.3299\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 16s 270us/step - loss: 0.1180 - val_loss: 0.3474\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 16s 270us/step - loss: 0.1148 - val_loss: 0.3609\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 16s 272us/step - loss: 0.1093 - val_loss: 0.3684\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 16s 274us/step - loss: 0.1028 - val_loss: 0.3758\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 16s 273us/step - loss: 0.1023 - val_loss: 0.3980\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 17s 275us/step - loss: 0.0967 - val_loss: 0.3822\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 16s 274us/step - loss: 0.0929 - val_loss: 0.4126\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 17s 288us/step - loss: 0.0914 - val_loss: 0.3951\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 18s 294us/step - loss: 0.0833 - val_loss: 0.4111\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 18s 297us/step - loss: 0.0820 - val_loss: 0.4259\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 18s 305us/step - loss: 0.0798 - val_loss: 0.4435\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 17s 278us/step - loss: 0.0776 - val_loss: 0.4523\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 17s 280us/step - loss: 0.0725 - val_loss: 0.4767\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 17s 279us/step - loss: 0.0732 - val_loss: 0.4558\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 17s 281us/step - loss: 0.0642 - val_loss: 0.4836\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 17s 281us/step - loss: 0.0640 - val_loss: 0.5003\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 18s 306us/step - loss: 0.0650 - val_loss: 0.5027\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 17s 281us/step - loss: 0.0599 - val_loss: 0.5857\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 16s 262us/step - loss: 0.0602 - val_loss: 0.5161\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 16s 262us/step - loss: 0.0561 - val_loss: 0.5440\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 16s 261us/step - loss: 0.0568 - val_loss: 0.5446\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 16s 261us/step - loss: 0.0542 - val_loss: 0.5509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa13bf382e8>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Compile and fit your model\n",
    "### STRIP_START ###\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "model = lenet5()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Define now our callbacks\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=10),\n",
    "             TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)]\n",
    "\n",
    "# Finally fit the model\n",
    "model.fit(x=X_train_norm, y=y_train_cat, validation_data=(X_test_norm, y_test_cat), epochs=100, batch_size=64, callbacks=callbacks)\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the tensorboard and see if it gives a deeper understanding of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute then the accuracy of your model. Is it better than a regular MLP used a couple of days ago?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on train with NN: 0.9768666666666667\n",
      "accuracy on test with NN: 0.8969\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute the accuracy of your model\n",
    "### STRIP_START ###\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_train = to_categorical(model.predict(X_train_norm).argmax(axis=1), num_classes=10)\n",
    "y_pred_test = to_categorical(model.predict(X_test_norm).argmax(axis=1), num_classes=10)\n",
    "\n",
    "print('accuracy on train with NN:', accuracy_score(y_pred_train, y_train_cat))\n",
    "print('accuracy on test with NN:', accuracy_score(y_pred_test, y_test_cat))\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now add image augmentation to improve our results, especially we will try to reduce overfitting this way.\n",
    "\n",
    "To do so, you can use `ImageDataGenerator` from Keras that makes all the work for you (including rescaling), with the following parameter: \n",
    "* `horizontal_flip=True`\n",
    "\n",
    "Begin by creating an object `ImageDataGenerator` with this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate an ImageDataGenerator object\n",
    "### STRIP_START ###\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(horizontal_flip=True)\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you have to fit your `ImageDataGenerator` on your training set of images before any preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fit your ImageDataGenerator object\n",
    "### STRIP_START ###\n",
    "datagen.fit(X_train_norm)\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can train your model using this generator, with the method `fit_generator` of your model and the method `flow` of your `ImageDataGenerator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "938/937 [==============================] - 14s 15ms/step - loss: 0.3191 - val_loss: 0.3322\n",
      "Epoch 2/100\n",
      "938/937 [==============================] - 16s 17ms/step - loss: 0.2126 - val_loss: 0.3395\n",
      "Epoch 3/100\n",
      "938/937 [==============================] - 15s 16ms/step - loss: 0.1931 - val_loss: 0.3277\n",
      "Epoch 4/100\n",
      "938/937 [==============================] - 15s 16ms/step - loss: 0.1790 - val_loss: 0.3396\n",
      "Epoch 5/100\n",
      "938/937 [==============================] - 15s 16ms/step - loss: 0.1728 - val_loss: 0.3396\n",
      "Epoch 6/100\n",
      "938/937 [==============================] - 16s 17ms/step - loss: 0.1658 - val_loss: 0.3365\n",
      "Epoch 7/100\n",
      "938/937 [==============================] - 17s 18ms/step - loss: 0.1584 - val_loss: 0.3509\n",
      "Epoch 8/100\n",
      "938/937 [==============================] - 18s 19ms/step - loss: 0.1505 - val_loss: 0.3436\n",
      "Epoch 9/100\n",
      "938/937 [==============================] - 17s 18ms/step - loss: 0.1481 - val_loss: 0.3690\n",
      "Epoch 10/100\n",
      "938/937 [==============================] - 15s 16ms/step - loss: 0.1440 - val_loss: 0.3439\n",
      "Epoch 11/100\n",
      "938/937 [==============================] - 16s 17ms/step - loss: 0.1405 - val_loss: 0.3551\n",
      "Epoch 12/100\n",
      "938/937 [==============================] - 16s 17ms/step - loss: 0.1336 - val_loss: 0.3643\n",
      "Epoch 13/100\n",
      "938/937 [==============================] - 15s 16ms/step - loss: 0.1319 - val_loss: 0.3693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa13bf38a90>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: train your model\n",
    "### STRIP_START ###\n",
    "batch_size = 64\n",
    "model.fit_generator(datagen.flow(X_train_norm, y_train_cat, batch_size=batch_size),\n",
    "                    validation_data=(X_test_norm, y_test_cat), callbacks=callbacks,\n",
    "                    steps_per_epoch=len(X_train_norm) / batch_size, epochs=100)\n",
    "\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recompute the accuracy of your model, does it improve your performances with data augmentation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on train with NN: 0.9903\n",
      "accuracy on test with NN: 0.8615\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute the accuracy of your model\n",
    "### STRIP_START ###\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_train = to_categorical(model.predict(X_train_norm).argmax(axis=1), num_classes=10)\n",
    "y_pred_test = to_categorical(model.predict(X_test_norm).argmax(axis=1), num_classes=10)\n",
    "\n",
    "print('accuracy on train with NN:', accuracy_score(y_pred_train, y_train_cat))\n",
    "print('accuracy on test with NN:', accuracy_score(y_pred_test, y_test_cat))\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now try to improve even more your results. For example, add more parameters to your `ImageDataGenerator`, play with some hyperparameters, and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
