{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-Movie-Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.unsplash.com/photo-1524985069026-dd778a71c7b4?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1051&q=80)\n",
    "\n",
    "Photo by [Erik Witsoe](https://unsplash.com/photos/GF8VvBgcJ4o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will compare the classical NLP approach to the sequential approach on the movie dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the dataset, located in `tensorflow.keras.datasets.imdb` with 10000 words (if you are experiencing memory issue, you can go down to 5000 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Load the dataset\n",
    "### STRIP_START ###\n",
    "from tensorflow.keras import datasets\n",
    "imdb = datasets.imdb\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=5000)\n",
    "\n",
    "X_train.shape, y_train.shape\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the dataset: you can make use of the function `imdb.get_word_index()` to get back to words and display some reviews. Be careful, the word indices `0`, `1`, `2` and `3` are reserved and mean no word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert is an amazing actor and now the same being director father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for and would recommend it to everyone to watch and the fly was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also to the two little that played the of norman and paul they were just brilliant children are often left out of the list i think because the stars that play them all grown up are such a big for the whole film but these children are amazing and should be for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was with us all\n"
     ]
    }
   ],
   "source": [
    "# TODO: Explore the data, display some sentences\n",
    "### STRIP_START ###\n",
    "imdb.get_word_index()\n",
    "\n",
    "index_to_word = dict([(value, key) for (key, value) in imdb.get_word_index().items()])\n",
    "\n",
    "output = [index_to_word[w-3] for w in X_train[0] if w>2]\n",
    "\n",
    "print(' '.join(output))\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a prediction using classical NLP tools: BOW and TF-IDF. Followed by a classification model. Choose a random forest or gradient boosting, and perform a grid search for hyperparameter optimization.\n",
    "\n",
    "*Warning, you are used to manipulate words, here they are already encoded into integers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Perform classification using NLP tools\n",
    "### STRIP_START ###\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer=(lambda x:x))\n",
    "\n",
    "tfidf.fit(X_train)\n",
    "\n",
    "X_train = tfidf.transform(X_train).toarray()\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Perform classification using NLP tools\n",
    "### STRIP_START ###\n",
    "# Here using only the train dataset for memory reasons, non mandatory step\n",
    "X_test = X_train[20000:]\n",
    "y_test = y_train[20000:]\n",
    "X_train = X_train[:20000]\n",
    "y_train = y_train[:20000]\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vince/.local/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO: Perform classification using NLP tools\n",
    "### STRIP_START ###\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on train: 0.99275\n",
      "accuracy on test: 0.7706\n"
     ]
    }
   ],
   "source": [
    "### TODO: Perform classification using NLP tools\n",
    "### STRIP_START ###\n",
    "print(\"accuracy on train:\", rf.score(X_train, y_train))\n",
    "print(\"accuracy on test:\", rf.score(X_test, y_test))\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'max_depth': None, 'n_estimators': 30}\n",
      "accuracy on train: 1.0\n",
      "accuracy on test: 0.8142\n"
     ]
    }
   ],
   "source": [
    "### TODO: Perform classification using NLP tools\n",
    "### STRIP_START ###\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_estimators': [10, 30],\n",
    "              'max_depth': [None, 5, 10]}\n",
    "\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=3)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"best params:\", grid.best_params_)\n",
    "print(\"accuracy on train:\", grid.score(X_train, y_train))\n",
    "print(\"accuracy on test:\", grid.score(X_test, y_test))\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What accuracy did you reach? Let's see if we can do better with RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you will use sequences, you will have to choose a sequence length.\n",
    "\n",
    "First, you can check the min, max and average length of the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min length: 11\n",
      "max length: 2494\n",
      "mean length: 238.71364\n",
      "median length: 178.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: compute basic descriptive statistics of the length of sequences\n",
    "### STRIP_START ###\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=5000)\n",
    "\n",
    "lengths = [len(seq) for seq in X_train]\n",
    "print('min length:', np.min(lengths))\n",
    "print('max length:', np.max(lengths))\n",
    "print('mean length:', np.mean(lengths))\n",
    "print('median length:', np.median(lengths))\n",
    "\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make now the padding of sequences: you choose a value related to the mean or median length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make the padding\n",
    "### STRIP_START ###\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train,\n",
    "                                 value=0,\n",
    "                                 padding='post', # to add zeros at the end\n",
    "                                 maxlen=128) # the length we want\n",
    "\n",
    "# Step done here to compare performances on the same data, not necessary if enough memory...\n",
    "X_test = X_train[20000:]\n",
    "y_test = y_train[20000:]\n",
    "X_train = X_train[:20000]\n",
    "y_train = y_train[:20000]\n",
    "\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build a RNN, with for example two layers of 32 units. Do not forget the first layer of embedding, and the last layer of sigmoid for binary classification. Warning, the training might take several minutes! You can choose to have less layers and/or units!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build your model\n",
    "### STRIP_START ###\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding, Dropout\n",
    "\n",
    "\n",
    "def my_RNN():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=5000, output_dim=32, input_length=128))\n",
    "    model.add(SimpleRNN(units=16, return_sequences=False))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally compile and train the model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "20000/20000 [==============================] - 6s 294us/sample - loss: 0.5186 - acc: 0.7474 - val_loss: 0.4194 - val_acc: 0.8306\n",
      "Epoch 2/30\n",
      "20000/20000 [==============================] - 7s 329us/sample - loss: 0.3466 - acc: 0.8602 - val_loss: 0.4415 - val_acc: 0.7986\n",
      "Epoch 3/30\n",
      "20000/20000 [==============================] - 7s 364us/sample - loss: 0.2913 - acc: 0.8827 - val_loss: 0.3484 - val_acc: 0.8578\n",
      "Epoch 4/30\n",
      "20000/20000 [==============================] - 7s 330us/sample - loss: 0.2524 - acc: 0.9042 - val_loss: 0.3499 - val_acc: 0.8554\n",
      "Epoch 5/30\n",
      "20000/20000 [==============================] - 7s 358us/sample - loss: 0.2202 - acc: 0.9144 - val_loss: 0.4175 - val_acc: 0.8510\n",
      "Epoch 6/30\n",
      "20000/20000 [==============================] - 7s 325us/sample - loss: 0.1884 - acc: 0.9310 - val_loss: 0.4128 - val_acc: 0.8522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7a975b0e48>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Compile and fit your model\n",
    "### STRIP_START ###\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "optimizer = optimizers.Adam(lr=0.005)\n",
    "\n",
    "model = my_RNN()\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define now our callbacks\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=3),\n",
    "             TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)]\n",
    "\n",
    "model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=30, batch_size=64, callbacks=callbacks)\n",
    "\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can have a look at the tensorboard as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 164us/sample - loss: 0.1345 - acc: 0.9556\n",
      "accuracy on train with NN: 0.9556\n",
      "5000/5000 [==============================] - 1s 195us/sample - loss: 0.4128 - acc: 0.8522\n",
      "accuracy on test with NN: 0.8522\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute the accuracy of your model\n",
    "### STRIP_START ###\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('accuracy on train with NN:', model.evaluate(X_train, y_train)[1])\n",
    "print('accuracy on test with NN:', model.evaluate(X_test, y_test)[1])\n",
    "### STRIP_END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to improve your results by playing with the hyperparameters: play with the layers and number of units, you can add dropout, play with the optimizer, mini-batch size, data preprocessing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
